# -*- coding: utf-8 -*-
"""【公開用】04章_コピペで簡単実行！キテレツおもしろ自然言語処理_付録コード.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ib6nqzBxTiN6hHOLLqgVA4c1SbAy3fR7
"""



"""ここに掲載されているコードは、  
『コピペで簡単実行！キテレツおもしろ自然言語処理～PythonとColaboratoryで身につく基礎の基礎』書籍の付録です。    
本ファイルの公開用URLや、本ファイル内のコードを、みだりに他の人へ共有しないでください。  

本ファイルは、書籍に記載のコードのコピペの手間を省くために作られました。  
本ファイルの公開用URLにアクセスすると、  
お手元のGoogleアカウントに本ファイルのコピーが作成されます。  
（つまり、コレをご覧になっているということは、コピーをご覧になっています）    
そのコピーは、あなただけのファイルとして生成されるため、    
書籍をお買い求め済みの方は、自由に変更したり実行したり保存したりして問題ありません。   

初期状態では、ご参考までに筆者が実行した出力結果を残してあります。  
ご自身で実行する際には、「編集」⇒「出力を全て消去」により、  
出力結果を消去してから実行なさると良いでしょう。  

※注：Googleの公式が作成したファイルではないため、実行時に  
「警告: このノートブックは Google が作成したものではありません。」  
という警告が出ます。  
気になる方は、ご自身の環境に都度コードをコピーして実行しても良いでしょう。



"""



"""■ 無限の猿の実装例："""

import random
# ランダムに選択するための文字のリストを作る
# hiragana_list = ["あ","い",・・・] というのと同じ
hiragana_list = [chr(i) for i in range(12353, 12436)]
# abc_list = [chr(i) for i in range(97, 97+26)]

kurikaesi_kaisuu = 1000
# 無限は厳しいので、指定の回数分繰り返しにしている
for counter in range(kurikaesi_kaisuu):
  # 改行なしで、リストの中からランダムにとった1文字を出力
  print(random.choice(hiragana_list), end='')

"""【※ ここで付録を参照し、記載のコードを実行してください。】  
の、付録のコード  
■ 青空文庫からのダウンロード＆加工用共通コード：  

"""

# 青空文庫からのダウンロードzip展開＆テキスト抽出

import re
import zipfile
import urllib.request
import os.path,glob
# 青空文庫のURLから小説テキストデータを得る関数
def get_flat_text_from_aozora(zip_url):
  # zipファイル名の取得
  zip_file_name = re.split(r'/', zip_url)[-1]
  print(zip_file_name)
  
  # 既にダウンロード済みか確認後、URLからファイルを取得
  if not os.path.exists(zip_file_name):
    print('Download URL = ',zip_url)
    data = urllib.request.urlopen(zip_url).read()
    with open(zip_file_name, mode="wb") as f:
      f.write(data)
  else:
    print('May be already exists')
  
  # 拡張子を除いた名前で、展開用フォルダを作成
  dir, ext = os.path.splitext(zip_file_name)
  if not os.path.exists(dir):
    os.makedirs(dir)
  
  # zipファイルの中身を全て、展開用フォルダに展開
  unzipped_data = zipfile.ZipFile(zip_file_name, 'r')
  unzipped_data.extractall(dir)
  unzipped_data.close()
  
  # zipファイルの削除
  os.remove(zip_file_name)
  # 注：展開フォルダの削除は入れていない
  
  # テキストファイル(.txt)の抽出
  wild_path = os.path.join(dir,'*.txt')
  # テキストファイルは原則1つ同梱。最初の1つを取得
  txt_file_path = glob.glob(wild_path)[0]

  print(txt_file_path)
  # 青空文庫はshift_jisのためデコードしてutf8にする
  binary_data = open(txt_file_path, 'rb').read()
  main_text = binary_data.decode('shift_jis')

  # 取得したutf8のテキストデータを返す
  return main_text


# 青空文庫のデータを加工して扱いやすくするコード

# 外字データ変換のための準備
# 外字変換のための対応表（jisx0213対応表）のダウンロード
# ※事前にダウンロード済みであれば飛ばしてもよい
!wget http://x0213.org/codetable/jisx0213-2004-std.txt

import re

# 外字変換のための対応表（jisx0213対応表）の読み込み
with open('jisx0213-2004-std.txt') as f:
  #ms = (re.match(r'(\d-\w{4})\s+U\+(\w{4})', l) for l in f if l[0] != '#')
  # 追加：jisx0213-2004-std.txtには5桁のUnicodeもあるため対応
  ms = (re.match(r'(\d-\w{4})\s+U\+(\w{4,5})', l) for l in f if l[0] != '#')
  gaiji_table = {m[1]: chr(int(m[2], 16)) for m in ms if m}

# 外字データの置き換えのための関数
def get_gaiji(s):
  # ※［＃「弓＋椁のつくり」、第3水準1-84-22］の形式を変換
  m = re.search(r'第(\d)水準\d-(\d{1,2})-(\d{1,2})', s)
  if m:
    key = f'{m[1]}-{int(m[2])+32:2X}{int(m[3])+32:2X}'
    return gaiji_table.get(key, s)
  # ※［＃「身＋單」、U+8EC3、56-1］の形式を変換
  m = re.search(r'U\+(\w{4})', s)
  if m:
    return chr(int(m[1], 16))
  # ※［＃二の字点、1-2-22］、［＃感嘆符二つ、1-8-75］の形式を変換
  m = re.search(r'.*?(\d)-(\d{1,2})-(\d{1,2})', s)
  if m:
    key = f'{int(m[1])+2}-{int(m[2])+32:2X}{int(m[3])+32:2X}'
    return gaiji_table.get(key, s)
  # 不明な形式の場合、元の文字列をそのまま返す
  return s

# 青空文庫の外字データ置き換え＆注釈＆ルビ除去などを行う加工関数
def flatten_aozora(text):
  # textの外字データ表記を漢字に置き換える処理
  text = re.sub(r'※［＃.+?］', lambda m: get_gaiji(m[0]), text)
  # 注釈文や、ルビなどの除去
  text = re.split(r'\-{5,}', text)[2]
  text = re.split(r'底本：', text)[0]
  text = re.sub(r'《.+?》', '', text)
  text = re.sub(r'［＃.+?］', '', text)
  text = text.strip()
  return text


# 複数ファイルのダウンロードや加工を一括実行する関数

import time
# ZIP-URLのリストから全てのデータをダウンロード＆加工する関数
def get_all_flat_text_from_zip_list(zip_list):
  all_flat_text = ""
  for zip_url in zip_list: 
    # ダウンロードや解凍の失敗があり得るためTry文を使う
    # 十分なデータ量があるため数件の失敗はスキップでよい
    try:
      # 青空文庫からダウンロードする関数を実行
      aozora_dl_text = get_flat_text_from_aozora(zip_url)
      # 青空文庫のテキストを加工する関数を実行
      flat_text = flatten_aozora(aozora_dl_text) 
      # 結果を追記して改行。
      all_flat_text += flat_text + ("\n")
      print(zip_url+" : 取得＆加工完了")
    except:
      # エラー時の詳細ログが出るおまじない
      import traceback
      traceback.print_exc()
      print(zip_url+" : 取得or解凍エラーのためスキップ")
    
    # 青空文庫サーバに負荷をかけすぎないように１秒待ってから次の小説へ
    time.sleep(1)
  
  # 全部がつながった大きなテキストデータを返す
  return all_flat_text

"""■ BeautifulSoupのインストールコマンド
（Colaboratoryで実行している場合は不要）
"""

!pip install beautifulsoup4

"""■ BeautifulSoupでリンク情報を取得するサンプル："""

import requests
from bs4 import BeautifulSoup

# Webページを取得して解析する
load_url = "https://www.aozora.gr.jp/index_pages/person9.html"
html = requests.get(load_url)
soup = BeautifulSoup(html.content, "html.parser")

# すべてのaタグを検索して、その文字列を表示する
for element in soup.find_all("a"):
  print(element)

"""■ 江戸川＆コナン全作品のZIP-URLを全て集めるコード："""

import requests
from bs4 import BeautifulSoup
import re
import urllib.parse

# <a href>タグのリンク先URL（絶対URL）を全て取得する関数
def get_a_href_list_from_url(load_url):
  html = requests.get(load_url)
  soup = BeautifulSoup(html.content, "html.parser")
  result_url_list = []
  for a_element in soup.find_all("a"):
    # 空だった場合は次のエレメントへ継続
    if a_element == None:
      continue
    # 各href属性を取得する
    link_str = a_element.get("href")
    # 空だった場合は次のエレメントへ継続
    if link_str == None:
      continue

    # "../cards/000009/card50713.html" などは
    # 元のURL（load_url）からのリンクとして相対的参照になっているため、
    # 元のURL+相対URLを入れると絶対URLを返してくれるライブラリを使用して加工する
    # urllib.parse.urljoin("http://www.example.com/foo/bar.html", "../hoge/fuga.html")
    # ⇒ http://www.example.com/hoge/fuga.html
    abs_url = urllib.parse.urljoin(load_url, link_str)

    # 取得した絶対URLを結果リストへ追加
    result_url_list.append(abs_url)
  return result_url_list

# 作者ページのURLを入力すると、ページを全探索して
# その作者の作品のzipファイルのURL一覧を返す関数
# 作者ページのURL = "https://www.aozora.gr.jp/index_pages/person9.html" など
def sakusyaurl2zipurllist(sakusya_url):
  # 作者ページから出ている全てのリンク先URLを取得
  url_list_from_sakusya = get_a_href_list_from_url(sakusya_url)

  # 図書カード＝作品ごとのページ、のURLを探す
  # 末尾が、 /cards/000009/card50713.html　のような形式になっている
  tosyo_card_url_list = []
  for tosyo_card_url in url_list_from_sakusya:
    # 条件に一致する場合（cardのURLの場合）のみリストに追加
    if re.match(r'.*cards.*card.*\.html', tosyo_card_url):
      tosyo_card_url_list.append(tosyo_card_url)

  # 図書カード＝作品ごとのページ に再度スクレイピングでアクセスして、
  # そのリンク先を全て取得
  # https://www.aozora.gr.jp/cards/000082/files/1293_ruby_5382.zip
  # のように、青空文庫内のZIPファイルへのアクセスになっている箇所を取得する
  zip_url_list = []
  for tosyo_card_url in tosyo_card_url_list:
    # 図書カードから出ている全てのリンク先URLを取得
    for zip_url in get_a_href_list_from_url(tosyo_card_url):
      # 条件に一致する場合（zipのURLの場合）のみリストに追加
      if re.match(r'.*aozora.*ruby.*\.zip', zip_url):
        zip_url_list.append( zip_url )

  # 取得したzipファイルの絶対URL一覧を返す
  return zip_url_list

# 江戸川乱歩の全作品のzipファイルのURLリスト
edogawa_zip_list = sakusyaurl2zipurllist("https://www.aozora.gr.jp/index_pages/person1779.html")
# 実際に取得できたリストを書き出す
print(edogawa_zip_list)
# 取得したリストの個数を書き出す
print(len(edogawa_zip_list))

# コナン・ドイルの全作品のzipファイルのURLリスト
konan_zip_list = sakusyaurl2zipurllist("https://www.aozora.gr.jp/index_pages/person9.html")
# 実際に取得できたリストを書き出す
print(konan_zip_list)
# 取得したリストの個数を書き出す
print(len(konan_zip_list))

"""■ 全小説データのダウンロード＆加工＆保存："""

# 江戸川乱歩の全データの取得＆加工
edogawa_all_text = get_all_flat_text_from_zip_list(edogawa_zip_list)
# 得た結果をファイルに書き込む
with open('edogawa_all_text.txt', 'w') as f:
  print(edogawa_all_text, file=f)
  print("★江戸川ALLファイル出力完了")

# コナン・ドイルの全データの取得＆加工
# ※超例外的に１件だけ別作家の小説をリンク/紹介しているため、除外
konan_zip_list.remove("https://www.aozora.gr.jp/cards/000082/files/1293_ruby_5382.zip")
konan_all_text = get_all_flat_text_from_zip_list(konan_zip_list)
# 得た結果をファイルに書き込む
with open('konan_all_text.txt', 'w') as f:
  print(konan_all_text, file=f)
  print("★コナンALLファイル出力完了")

# 江戸川＆コナンの両方の全テキストをつなげたファイルも作っておく
edogawa_konan_all_text = edogawa_all_text + konan_all_text
with open('edogawa_konan_all_text.txt', 'w') as f:
  print(edogawa_konan_all_text, file=f)
  print("★江戸川＆コナンALLファイル出力完了")

"""■ Janomeのインストールコマンド：  

"""

!pip install janome

"""■ 分かち書きを行う関数： """

# Janomeのロード
from janome.tokenizer import Tokenizer

# Tokenizerインスタンスの生成 
tokenizer = Tokenizer()

# 文章を入れると、単語のリストにする関数
# Janomeの wakati = True オプションを使う方法もあるが、
# これまで使ってきているのと同様の形式で実装
def make_wakati_list(input_str):
  result_list = []
  tokens = tokenizer.tokenize(input_str)
  for token in tokens:
    # 元の単語＋半角スペースを追加しているため、
    # 結果とした、単語の切れ目全てに半角スペースが入る
    result_list.append(token.surface)
  return result_list

print(make_wakati_list("この文章を単語の切れ目で区切ってみよう。"))

"""■ マルコフ連鎖のデータを生成する：  

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # 元ネタとなる小説のテキストデータと、
# # マルコフ連鎖の単語数（チェインナンバー）を入れると、
# # マルコフ連鎖用の辞書を作成する関数
# def make_markov_dict(input_text_file_path, chain_number):
#   # マルコフ連鎖用の辞書
#   markov_dict = {}
# 
#   # readlinesで、テキストを読み込んで１行ごとにリスト化
#   with open(input_text_file_path) as f:
#     text_lines = f.readlines()
#     # print(text_lines)
# 
#   # １行ごとに処理してマルコフ連鎖用の辞書に追記していく
#   for one_line in text_lines:
#     # １行ごとに、改行コードやタブなどを消す（綺麗化前処理）
#     one_line = ''.join(one_line.splitlines())
#     # 形態素解析して、１行を、単語リストにする
#     word_list = make_wakati_list(one_line)
# 
#     # 単語リストの最初と最後に、文頭/文末を示すフラグを追加する
#     word_list = ["__BOS__"] +  word_list + ["__EOS__"]
# 
#     # 最低でもchain_number+1個の単語が残っている必要があり、
#     # word_listの単語数が十分な間は処理を繰り返す
#     while len(word_list) > chain_number:
#       # 最初の chain_number 個の単語を、辞書に登録する際のキーとする。
#       # （辞書のキーとして扱うため、tupleという形式に変換しておく）
#       # key = ("__BOF__", "この", "文章") value = "を" のような形で格納される。
#       # ループの２回目では、最初の"__BOF__"が削除されて繰り返されるため、
#       # key = ("この", "文章", "を") value = "単語" のような形になる。以下同様  
#       key = tuple( word_list[0 : chain_number] )
#       # 次に続く単語は、そのキーの次の単語
#       value = word_list[chain_number]
#       markov_dict
# 
#       # 初回登録の場合、そのkeyに対する空のリストを作る処理
#       # 既にそのkeyに対するデータがある場合は何もしない
#       markov_dict.setdefault(key, [] )
# 
#       # そのkeyに登録されているリストに、今回のvalueを追加する
#       markov_dict[key].append(value)
# 
#       # リストの最初の単語を削除する
#       # ※ここでだんだん単語数が減っていくため、いつかはループ処理を抜ける
#       word_list.pop(0)
#   
#   # 全ての行を処理し終わったら、完成した辞書データをリターン
#   return markov_dict
# 
# # コナン・ドイルの全作品からマルコフ連鎖用の辞書データを作成する
# konan_markov_dict = make_markov_dict("konan_all_text.txt", 3)
# 
#

"""■ 作成したマルコフ連鎖の辞書の中身の確認：  

"""

print(list(konan_markov_dict.items())[0:3])

"""■ マルコフ連鎖で文章を生成する：  """

import random

# マルコフ連鎖用の辞書と最初のキー候補リストを入れると文章を作る関数
def make_markov_sentence(markov_dict):
  # 出力用の文字列
  output_sentence = ""

  # 最大１万回ランダムに繰り返して冒頭を取得する
  key_list = list(markov_dict.keys())
  for a in range(10000):
    # 最初のキーをランダムに取得する
    key = random.choice(key_list)
    # 文頭のフラグが出るまで繰り返し
    if key[0] == "__BOS__":
      break
  
  # key(tuple型)を結合して文字列にして追記
  output_sentence += "".join(map(str, key))
  
  # 最大１万回繰り返し（通常は途中でbreakして終了）
  for a in range(10000):
    # keyに対応するvalue（次の単語候補のリスト）を取得
    value = markov_dict.get(key)
    # 例外処理：もしkeyが辞書に見つからない場合処理終了
    if value == None:
      break

    # 単語のリストから次の単語をランダムに選ぶ
    next_word = random.choice(value)
    # 文章に追記する
    output_sentence += next_word
    # 文末のフラグが出ていたら終了
    if next_word == "__EOS__":
      break
    
    # 既存キーの最初を除外して、next_wordをくっつけて新しいkeyに更新する
    # ※tupleの結合処理
    key = key[1:] + (next_word, )
  
  # 生成された文章を返す
  return output_sentence

# 何回か文章を生成してみる
for a in range(20):
  print(make_markov_sentence(konan_markov_dict))

"""■ 「江戸川＆コナン」を対象にして実行：  

"""

# 江戸川＆コナン両先生の全作品からマルコフ連鎖用の辞書データを作成する
edogawa_konan_markov_dict = make_markov_dict("edogawa_konan_all_text.txt", 3)

# 何回か文章を生成してみる
for a in range(20):
  print(make_markov_sentence(edogawa_konan_markov_dict))

