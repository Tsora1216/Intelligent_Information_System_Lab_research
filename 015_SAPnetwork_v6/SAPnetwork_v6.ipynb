{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "教授にプレゼン後、データの保存をし、テーブルとして値を持つ必要があることがわかった\n",
    "大きなデータになるので保存方法を考慮する必要があった。\n",
    "CSV、Excel、バイナリ、HDF5が候補として挙がった。\n",
    "HDF5形式は大容量のデータセットを効率的に保存するためのファイル形式であるため、採択。\n",
    "参考サイトの少なさから一時的にSQLを採択\n",
    "SQLをテストし、全語句のデータの保存をしてみた\n",
    "SQLでは、VisualStudioCodeの拡張機能が使いづらく、SQLite3に変更した\n",
    "SQLite->dataframe->変更->dataframe->変更を保存->SQLiteの完成\n",
    "上記の方法では変更件数に関わらず、全件で1分30秒かかる。\n",
    "変更に時間がかかりすぎな点が課題であるため、データの更新方法を検討\n",
    "SQLからPythonに値を渡さず、直接書き込むことで一件当たり0.1秒以内\n",
    "拡散ネットワークを作るために、その基盤となるデータセットをSQLに保存\n",
    "拡散ネットワークに対し、類似度の高いものを想起する事からそれらを加点するため拡散数を３と仮定し拡散\n",
    "また、拡散ネットワークを作るために、関連文字列に拡散し、ループ一回を拡散と見なしたとき、一回目の入力を除き、n=3の時,3,9,27のように、3^(n-1)のように増えていく。\n",
    "また実行時間は、0.7,0.8,0.8,4.0,5.9,15.9となっており、線形的に増えている\n",
    "プログラム自体の計算量はO(N^2)になっている。\n",
    "拡散した値を仮に0.3として全部をSQLに書き込む処理を追加→成功\n",
    "SQLから元の値を取得し、Scoreを加算していくことで活性値を表現\n",
    "活性値が閾値を超えたらループが止まるようにしたいが、5以上の数値は計算上現実的ではない\n",
    "活性値を超えたら。という考え方自体が並列処理でないため、同時に超えるはずが計算の手順により結果がランダムに決まってしまうことに気づいた。\n",
    "そのため、理論上下記の仕様に変更する必要がある\n",
    "活性値を超えたら→活性回数の上限回数を決め、最大の活性値を活性化とする。\n",
    "活性化の状態を棒グラフに出力していく\n",
    "保存先を\"%Y%m%d%H%M%S\"でフォーマットすることにより、読込先のデータベースが毎秒変化する仕様を加えた\n",
    "そのため、一回実行したらその後のデータは初期化される。しかし、dataframeでmodel内を走査しデータ基盤を作成するため毎回整った基盤の実現をしている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "# Get current time\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# Create output folder with the current time as part of the name\n",
    "folder_name = f\"./Output_{current_time}\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力\n",
    "input_str = input(\"Please enter string in SAPNet===>\")\n",
    "model_file_path = '../../KITERETU/gw2v160.model'\n",
    "\n",
    "# 出力\n",
    "database_path = folder_name + '/database.sqlite'\n",
    "Heatmap_path = folder_name + '/heatmap.png'\n",
    "Network_path = folder_name + '/network.png'\n",
    "GIF_path_100 = folder_name + '/graph_100.gif'\n",
    "GIF_path_1000 = folder_name + '/graph_1000.gif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "# 学習済みモデルのロード\n",
    "model = Word2Vec.load(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# データベースに接続し、カーソルを作成する\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# テーブルが存在しない場合のみ作成する\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS words (word TEXT, value REAL)''')\n",
    "\n",
    "# モデル内のすべての単語と値をデータベースに保存する\n",
    "for word in model.wv.key_to_index:\n",
    "    cursor.execute('''INSERT INTO words VALUES (?, ?)''', (word, 0.000000))\n",
    "\n",
    "# コミットして変更を確定する\n",
    "conn.commit()\n",
    "\n",
    "# データベース接続を閉じる\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         value\n",
      "word          \n",
      "其の         0.0\n",
      "使用         0.0\n",
      "1          0.0\n",
      "世紀         0.0\n",
      "遡る         0.0\n",
      "...        ...\n",
      "kienle     0.0\n",
      "アリアラテース    0.0\n",
      "ラポーサ       0.0\n",
      "pois       0.0\n",
      "デグレダード     0.0\n",
      "\n",
      "[293753 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# データベースに接続する\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "df = pd.read_sql_query(\"SELECT * FROM words\", conn)\n",
    "\n",
    "# キーを「word」列に変更する\n",
    "df = df.set_index('word')\n",
    "print(df)\n",
    "\n",
    "# データベース接続を閉じる\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293753\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key '' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index))\n\u001b[0;32m      4\u001b[0m \u001b[39m# 「群馬」に似ている単語TOP7を書き出す\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(positive\u001b[39m=\u001b[39;49m[input_str], topn\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(out)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key '' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# モデル内に登録されている単語リストの長さをlen関数で見る（＝単語数）\n",
    "print(len(model.wv.key_to_index))\n",
    "\n",
    "# 「群馬」に似ている単語TOP7を書き出す\n",
    "out = model.wv.most_similar(positive=[input_str], topn=10)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)\n",
    "\n",
    "#AtCoderでよく使う辞書処理を使って県と数値を取り出す\n",
    "pre=[]\n",
    "pre.append(input_str)\n",
    "for prefecture, value in out:\n",
    "    print(prefecture, value)\n",
    "    pre.append(prefecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.crosstab(pre, pre, rownames=['X'], colnames=['Y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pre:\n",
    "    for j in pre:\n",
    "        #0を除外する処理を入れていないので計算数がその分多い\n",
    "        if model.wv.similarity(i,j) >= 1.0:\n",
    "            df.loc[i,j]=0.0\n",
    "        elif model.wv.similarity(i,j) >= 0.6:\n",
    "            df.loc[i,j]=model.wv.similarity(i,j)\n",
    "        else:\n",
    "            df.loc[i,j]=0\n",
    "        #print(df.loc['Bob', 'age'])\n",
    "        #df.iloc[1,0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"入力文字列「{}」により想起された文字列の類似度ヒートマップ\".format(input_str))\n",
    "sns.heatmap(df,cmap=\"winter\")\n",
    "plt.savefig(Heatmap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "# ネットワークグラフの作成\n",
    "G = nx.from_pandas_adjacency(df, create_using=nx.Graph)\n",
    "\n",
    "# グラフの描画\n",
    "plt.figure(figsize=(len(input_str)+8, len(input_str)+6))\n",
    "pos = nx.spring_layout(G)  # ノードの位置を設定\n",
    "weights = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "# 辺の太さを重みに応じて調整\n",
    "max_weight = max(weights.values())\n",
    "edge_widths = [(6 * (1 - weights[edge])) ** 2 for edge in G.edges()]\n",
    "\n",
    "# ノードの太さを重みに応じて調整\n",
    "node_sizes = [50 + 150 * G.degree(node) for node in G.nodes()]\n",
    "\n",
    "# グラフを描画\n",
    "japanize_matplotlib.japanize()\n",
    "nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=node_sizes)\n",
    "nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='black')\n",
    "nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_color='black', font_size=10)\n",
    "plt.title('入力文字列「{}」により想起された文字列の類似度ネットワーク'.format(input_str))\n",
    "plt.axis('off')\n",
    "\n",
    "# グラフを出力\n",
    "plt.savefig(Network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd \n",
    "import sqlite3 \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image  # Import the PIL module \n",
    "import io \n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "from PIL import Image # Import the PIL module \n",
    "import glob # Import the glob module\n",
    "\n",
    " # Connect to the database \n",
    "conn = sqlite3.connect(database_path) \n",
    "cursor = conn.cursor() \n",
    "ans = [] \n",
    "ans.append([input_str])\n",
    "print(ans) \n",
    "images = [] \n",
    "for i in range(3): \n",
    "    ans.append([]) \n",
    "    for j in range(len(ans[i])): \n",
    "        out = model.wv.most_similar(positive=[ans[i][j]], topn=3) \n",
    "        for prefecture, value in out: \n",
    "            ans[i+1].append(prefecture) \n",
    "            # Execute SQL query to get the numerical value \n",
    "            cursor.execute(\"SELECT value FROM words WHERE word = ?\", (prefecture,)) \n",
    "            result = cursor.fetchone()[0] \n",
    "            new_result = result + value \n",
    "            val = '{:6f}'.format(new_result) \n",
    "            cursor.execute(\"UPDATE words SET value = ? WHERE word = ?\", (val, prefecture)) \n",
    "            # Execute SQL query to get the result \n",
    "            cursor.execute(\"SELECT word, value FROM words ORDER BY value DESC LIMIT 10\") \n",
    "            results = cursor.fetchall() \n",
    "            words = [result[0] for result in results] \n",
    "            values = [result[1] for result in results] \n",
    "            print(words) \n",
    "            print(values) \n",
    "            # Create the graph \n",
    "            plt.bar(words, values)  # Display words on the x-axis \n",
    "            plt.xlabel('Word') \n",
    "            plt.ylabel('Value') \n",
    "            # Create a PIL Image object from the plot \n",
    "            buf = io.BytesIO() \n",
    "            plt.savefig(buf, format='png') \n",
    "            buf.seek(0) \n",
    "            im = Image.open(buf) \n",
    "            images.append(im) \n",
    "            plt.clf()  # Clear the figure for the next iteration \n",
    "# Save the frames as a GIF image \n",
    "\n",
    "images[0].save(GIF_path_100, format='GIF', append_images=images[1:], save_all=True, duration=100, loop=0)\n",
    "images[0].save(GIF_path_1000, format='GIF', append_images=images[1:], save_all=True, duration=1000, loop=0)\n",
    "# Commit changes and close the database connection \n",
    "conn.commit() \n",
    "conn.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython import display as dd # display関数と名前が被るので別名にしておきます。\n",
    "\n",
    "with open(GIF_path_100, \"rb\") as f:\n",
    "    b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    \n",
    "display(dd.HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#出力A\n",
    "print(\"入力文字列「{}」\".format(input_str))\n",
    "import sqlite3\n",
    "\n",
    "#出力B\n",
    "# データベースに接続する\n",
    "conn = sqlite3.connect(database_path)\n",
    "# クエリを実行する\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT word, value FROM words ORDER BY value DESC LIMIT 10\")\n",
    "# 結果を取得する\n",
    "results = cur.fetchall()\n",
    "# 結果を出力する\n",
    "for word, value in results:\n",
    "    print(\"{:7s} \\t{:.6f}\".format(word, value))\n",
    "# データベースを切断する\n",
    "conn.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 画像ファイルを読み込む\n",
    "Heatmap_img = plt.imread(Heatmap_path)\n",
    "Network_img = plt.imread(Network_path)\n",
    "\n",
    "# グラフを描画\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(Heatmap_img)\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(Network_img)\n",
    "plt.show()\n",
    "\n",
    "#出力E\n",
    "import base64\n",
    "from IPython import display as dd # display関数と名前が被るので別名にしておきます。\n",
    "with open(GIF_path_100, \"rb\") as f:\n",
    "    b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "display(dd.HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))\n",
    "\n",
    "with open(GIF_path_1000, \"rb\") as f:\n",
    "    b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "display(dd.HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))\n",
    "\n",
    "# 結果を出力する\n",
    "i=1\n",
    "for word, value in results:\n",
    "    print(\"{}位\\t-> {:7s} \\t{:.6f}\".format(i,word, value))\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
